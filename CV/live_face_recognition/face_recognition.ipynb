{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook is to recognize faces on live camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khs/anaconda3/envs/face_recognition/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import mmcv, cv2\n",
    "from IPython import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing MTCNN and InceptionResnetV1 \n",
    "\n",
    "mtcnn0 = MTCNN(image_size=240, margin=0, keep_all=False, min_face_size=40) # keep_all=False\n",
    "mtcnn = MTCNN(image_size=240, margin=0, keep_all=True, min_face_size=40) # keep_all=True\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from folder\n",
    "\n",
    "dataset = datasets.ImageFolder('photos') # photos folder path \n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "name_list = [] # list of names corrospoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn0(img, return_prob=True) \n",
    "    if face is not None and prob>0.92:\n",
    "        emb = resnet(face.unsqueeze(0)) \n",
    "        embedding_list.append(emb.detach()) \n",
    "        name_list.append(idx_to_class[idx])        \n",
    "\n",
    "# save data\n",
    "data = [embedding_list, name_list] \n",
    "torch.save(data, 'data.pt') # saving data.pt file\n",
    "load_data = torch.load('data.pt') \n",
    "embedding_list = load_data[0] \n",
    "name_list = load_data[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"20221116_231147.mp4\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = mmcv.VideoReader('20221116_231147.mp4')\n",
    "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "\n",
    "display.Video('20221116_231147.mp4', width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame: 1[ 575.8802   131.19029 1051.7288   731.9874 ]\n",
      "Tracking frame: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6981/732748434.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  frames_tracked.append(frame.resize((640, 360), Image.BILINEAR))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 572.34534  131.15764 1056.7426   735.3607 ]\n",
      "Tracking frame: 3[ 578.365    128.54277 1057.0063   736.3305 ]\n",
      "Tracking frame: 4[ 583.20404  135.31259 1045.4138   732.4888 ]\n",
      "Tracking frame: 5[ 580.4615   135.75089 1035.497    723.6887 ]\n",
      "Tracking frame: 6[ 589.1375   139.70508 1035.0677   722.0658 ]\n",
      "Tracking frame: 7[ 592.9174   138.3352  1045.3624   728.99084]\n",
      "Tracking frame: 8[ 593.82184  136.09207 1044.5404   724.42365]\n",
      "Tracking frame: 9[ 587.61584  133.75525 1051.2443   731.4957 ]\n",
      "Tracking frame: 10[ 572.22815   113.998215 1062.5658    738.88544 ]\n",
      "Tracking frame: 11[ 599.51855  132.03777 1048.3478   724.03656]\n",
      "Tracking frame: 12[ 601.11584   123.912415 1060.1752    723.80383 ]\n",
      "Tracking frame: 13[ 603.9678   131.74454 1058.4926   721.17065]\n",
      "Tracking frame: 14[ 600.09766  120.4126  1062.4827   730.464  ]\n",
      "Tracking frame: 15[ 614.7779   132.41003 1071.5453   726.6289 ]\n",
      "Tracking frame: 16[ 626.15015  135.53456 1078.6624   730.9084 ]\n",
      "Tracking frame: 17[ 617.47974   127.719635 1086.6942    737.0818  ]\n",
      "Tracking frame: 18[ 635.2756   139.86841 1090.0732   729.35583]\n",
      "Tracking frame: 19[ 636.4521   122.35278 1096.7454   730.18207]\n",
      "Tracking frame: 20[ 634.0012   130.27579 1101.3905   740.83203]\n",
      "Tracking frame: 21[ 651.41144  131.71324 1105.8435   730.0879 ]\n",
      "Tracking frame: 22[ 656.92676  126.30655 1114.2028   731.5466 ]\n",
      "Tracking frame: 23[ 656.4513   128.22949 1115.9995   731.3781 ]\n",
      "Tracking frame: 24[ 662.8309  131.3284 1120.1901  731.6168]\n",
      "Tracking frame: 25"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khs/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/home/khs/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/home/khs/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/home/khs/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n",
      "/home/khs/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:250: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if method is \"Min\":\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000005?line=5'>6</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000005?line=6'>7</a>\u001b[0m \u001b[39m#img =frame\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000005?line=7'>8</a>\u001b[0m img_cropped_list, prob_list \u001b[39m=\u001b[39m mtcnn(img, return_prob\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000005?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m img_cropped_list \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000005?line=9'>10</a>\u001b[0m     boxes, _ \u001b[39m=\u001b[39m mtcnn\u001b[39m.\u001b[39mdetect(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:258\u001b[0m, in \u001b[0;36mMTCNN.forward\u001b[0;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39m\"\"\"Run MTCNN face detection on a PIL image or numpy array. This method performs both\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39mdetection and extraction of faces, returning tensors representing detected faces rather\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[39mthan the bounding boxes. To access bounding boxes, see the MTCNN.detect() method below.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m>>> face_tensor, prob = mtcnn(img, save_path='face.png', return_prob=True)\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# Detect faces\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m batch_boxes, batch_probs, batch_points \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect(img, landmarks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    259\u001b[0m \u001b[39m# Select faces\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_all:\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[0;34m(self, img, landmarks)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[39mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[39m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[39m=\u001b[39m detect_face(\n\u001b[1;32m    314\u001b[0m         img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_face_size,\n\u001b[1;32m    315\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnet, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monet,\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mthresholds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfactor,\n\u001b[1;32m    317\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    318\u001b[0m     )\n\u001b[1;32m    320\u001b[0m boxes, probs, points \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m    321\u001b[0m \u001b[39mfor\u001b[39;00m box, point \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:117\u001b[0m, in \u001b[0;36mdetect_face\u001b[0;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[1;32m    114\u001b[0m im_data \u001b[39m=\u001b[39m (im_data \u001b[39m-\u001b[39m \u001b[39m127.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m0.0078125\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[39m# This is equivalent to out = rnet(im_data) to avoid GPU out of memory.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m out \u001b[39m=\u001b[39m fixed_batch_process(im_data, rnet)\n\u001b[1;32m    119\u001b[0m out0 \u001b[39m=\u001b[39m out[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    120\u001b[0m out1 \u001b[39m=\u001b[39m out[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/utils/detect_face.py:21\u001b[0m, in \u001b[0;36mfixed_batch_process\u001b[0;34m(im_data, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(im_data), batch_size):\n\u001b[1;32m     20\u001b[0m     batch \u001b[39m=\u001b[39m im_data[i:(i\u001b[39m+\u001b[39mbatch_size)]\n\u001b[0;32m---> 21\u001b[0m     out\u001b[39m.\u001b[39mappend(model(batch))\n\u001b[1;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mcat(v, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mout))\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/facenet_pytorch/models/mtcnn.py:88\u001b[0m, in \u001b[0;36mRNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n\u001b[1;32m     87\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprelu2(x)\n\u001b[0;32m---> 88\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool2(x)\n\u001b[1;32m     89\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)\n\u001b[1;32m     90\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprelu3(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/face_recognition/lib/python3.8/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frames_tracked = []\n",
    "for i, frame in enumerate(frames):\n",
    "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
    "    #print(frame)\n",
    "    x = np.array(frame)\n",
    "    img = Image.fromarray(x)\n",
    "    #img =frame\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "\n",
    "                frame = np.array(frame)\n",
    "                if min_dist<0.9:\n",
    "                    frame = cv2.putText(frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                    print(box)\n",
    "                    #draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                #draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                #frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "                frame = Image.fromarray(frame, 'RGB')\n",
    "                frames_tracked.append(frame.resize((640, 360), Image.BILINEAR))\n",
    "                #print('tracked')\n",
    "#                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "    '''         \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "    \n",
    "    # Draw faces\n",
    "    frame_draw = frame.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "    \n",
    "    # Add to frame list\n",
    "    frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))\n",
    "    '''\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nd = display.display(frames_tracked[0], display_id=True)\\ni = 1\\ntry:\\n    while True:\\n        d.update(frames_tracked[i % len(frames_tracked)])\\n        i += 1\\nexcept KeyboardInterrupt:\\n    pass\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "d = display.display(frames_tracked[0], display_id=True)\n",
    "i = 1\n",
    "try:\n",
    "    while True:\n",
    "        d.update(frames_tracked[i % len(frames_tracked)])\n",
    "        i += 1\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "dim = frames_tracked[0].size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
    "video_tracked = cv2.VideoWriter('video_tracked.mp4', fourcc, 25.0, dim)\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n",
    "video_tracked.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"input.avi\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "video = mmcv.VideoReader('input.avi')\n",
    "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "\n",
    "display.Video('input.avi', width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb 셀 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000009?line=0'>1</a>\u001b[0m frames_tracked \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, frame \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(frames):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000009?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mTracking frame: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/khs/Documents/TNT/cv_project/22-2-Computer-Vision/CV/live_face_recognition/face_recognition.ipynb#ch0000009?line=3'>4</a>\u001b[0m     \u001b[39m#print(frame)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frames' is not defined"
     ]
    }
   ],
   "source": [
    "frames_tracked = []\n",
    "for i, frame in enumerate(frames):\n",
    "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
    "    #print(frame)\n",
    "    x = np.array(frame)\n",
    "    img = Image.fromarray(x)\n",
    "    #img =frame\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "\n",
    "                frame = np.array(frame)\n",
    "                if min_dist<0.9:\n",
    "                    frame = cv2.putText(frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                    \n",
    "                    print('\\n' + box)\n",
    "                    #draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                #draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                #frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "                frame = Image.fromarray(frame, 'RGB')\n",
    "                frames_tracked.append(frame.resize((640, 360), Image.BILINEAR))\n",
    "                #print('tracked')\n",
    "#                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "    '''         \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "    \n",
    "    # Draw faces\n",
    "    frame_draw = frame.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "    \n",
    "    # Add to frame list\n",
    "    frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))\n",
    "    '''\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "dim = frames_tracked[0].size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
    "video_tracked = cv2.VideoWriter('video_tracked_2.mp4', fourcc, 25.0, dim)\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n",
    "video_tracked.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('face_recognition')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fdd09f6a44b62e78a4e7154e2dfc62e27023bfdf643f1270facf69ba937d002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
