{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This jupyter notebook is to recognize faces on live camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khs/anaconda3/envs/face_recognition/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import mmcv, cv2\n",
    "from IPython import display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing MTCNN and InceptionResnetV1 \n",
    "\n",
    "mtcnn0 = MTCNN(image_size=240, margin=0, keep_all=False, min_face_size=40) # keep_all=False\n",
    "mtcnn = MTCNN(image_size=240, margin=0, keep_all=True, min_face_size=40) # keep_all=True\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from folder\n",
    "\n",
    "dataset = datasets.ImageFolder('photos') # photos folder path \n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "name_list = [] # list of names corrospoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn0(img, return_prob=True) \n",
    "    if face is not None and prob>0.92:\n",
    "        emb = resnet(face.unsqueeze(0)) \n",
    "        embedding_list.append(emb.detach()) \n",
    "        name_list.append(idx_to_class[idx])        \n",
    "\n",
    "# save data\n",
    "data = [embedding_list, name_list] \n",
    "torch.save(data, 'data.pt') # saving data.pt file\n",
    "load_data = torch.load('data.pt') \n",
    "embedding_list = load_data[0] \n",
    "name_list = load_data[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"20221116_231147.mp4\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video = mmcv.VideoReader('20221116_231147.mp4')\n",
    "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "\n",
    "display.Video('20221116_231147.mp4', width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame: 1success\n",
      "Tracking frame: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5667/3818580657.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  frames_tracked.append(frame.resize((640, 360), Image.BILINEAR))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "Tracking frame: 3success\n",
      "Tracking frame: 4success\n",
      "Tracking frame: 5success\n",
      "Tracking frame: 6success\n",
      "Tracking frame: 7success\n",
      "Tracking frame: 8success\n",
      "Tracking frame: 9success\n",
      "Tracking frame: 10success\n",
      "Tracking frame: 11success\n",
      "Tracking frame: 12success\n",
      "Tracking frame: 13success\n",
      "Tracking frame: 14success\n",
      "Tracking frame: 15success\n",
      "Tracking frame: 16success\n",
      "Tracking frame: 17success\n",
      "Tracking frame: 18success\n",
      "Tracking frame: 19success\n",
      "Tracking frame: 20success\n",
      "Tracking frame: 21success\n",
      "Tracking frame: 22success\n",
      "Tracking frame: 23success\n",
      "Tracking frame: 24success\n",
      "Tracking frame: 25success\n",
      "Tracking frame: 26success\n",
      "Tracking frame: 27success\n",
      "Tracking frame: 28success\n",
      "Tracking frame: 29success\n",
      "Tracking frame: 30success\n",
      "Tracking frame: 31success\n",
      "Tracking frame: 32success\n",
      "Tracking frame: 33success\n",
      "Tracking frame: 34success\n",
      "Tracking frame: 35success\n",
      "Tracking frame: 36success\n",
      "Tracking frame: 37success\n",
      "Tracking frame: 38success\n",
      "Tracking frame: 39success\n",
      "Tracking frame: 40success\n",
      "Tracking frame: 41success\n",
      "Tracking frame: 42success\n",
      "Tracking frame: 43success\n",
      "Tracking frame: 44success\n",
      "Tracking frame: 45success\n",
      "Tracking frame: 46success\n",
      "Tracking frame: 47success\n",
      "Tracking frame: 48success\n",
      "Tracking frame: 49success\n",
      "Tracking frame: 50success\n",
      "Tracking frame: 51success\n",
      "Tracking frame: 52success\n",
      "Tracking frame: 53success\n",
      "Tracking frame: 54success\n",
      "Tracking frame: 55success\n",
      "Tracking frame: 56success\n",
      "Tracking frame: 57success\n",
      "Tracking frame: 58success\n",
      "Tracking frame: 59success\n",
      "Tracking frame: 60success\n",
      "Tracking frame: 61success\n",
      "Tracking frame: 62success\n",
      "Tracking frame: 63success\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "frames_tracked = []\n",
    "for i, frame in enumerate(frames):\n",
    "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
    "    #print(frame)\n",
    "    x = np.array(frame)\n",
    "    img = Image.fromarray(x)\n",
    "    #img =frame\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "\n",
    "                frame = np.array(frame)\n",
    "                if min_dist<0.9:\n",
    "                    frame = cv2.putText(frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                    print('success')\n",
    "                    #draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                #draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                #frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "                frame = Image.fromarray(frame, 'RGB')\n",
    "                frames_tracked.append(frame.resize((640, 360), Image.BILINEAR))\n",
    "                #print('tracked')\n",
    "#                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "    '''         \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "    \n",
    "    # Draw faces\n",
    "    frame_draw = frame.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "    \n",
    "    # Add to frame list\n",
    "    frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))\n",
    "    '''\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nd = display.display(frames_tracked[0], display_id=True)\\ni = 1\\ntry:\\n    while True:\\n        d.update(frames_tracked[i % len(frames_tracked)])\\n        i += 1\\nexcept KeyboardInterrupt:\\n    pass\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "d = display.display(frames_tracked[0], display_id=True)\n",
    "i = 1\n",
    "try:\n",
    "    while True:\n",
    "        d.update(frames_tracked[i % len(frames_tracked)])\n",
    "        i += 1\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "dim = frames_tracked[0].size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
    "video_tracked = cv2.VideoWriter('video_tracked.mp4', fourcc, 25.0, dim)\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n",
    "video_tracked.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"input.avi\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "video = mmcv.VideoReader('input.avi')\n",
    "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
    "\n",
    "display.Video('input.avi', width=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame: 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5667/3818580657.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  frames_tracked.append(frame.resize((640, 360), Image.BILINEAR))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame: 105\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "frames_tracked = []\n",
    "for i, frame in enumerate(frames):\n",
    "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
    "    #print(frame)\n",
    "    x = np.array(frame)\n",
    "    img = Image.fromarray(x)\n",
    "    #img =frame\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.90:\n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "\n",
    "                frame = np.array(frame)\n",
    "                if min_dist<0.9:\n",
    "                    frame = cv2.putText(frame, name+' '+str(min_dist), (int(box[0]),int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
    "                    print('success')\n",
    "                    #draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                #draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "                #frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "                frame = Image.fromarray(frame, 'RGB')\n",
    "                frames_tracked.append(frame.resize((640, 360), Image.BILINEAR))\n",
    "                #print('tracked')\n",
    "#                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "    '''         \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame)\n",
    "    \n",
    "    # Draw faces\n",
    "    frame_draw = frame.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    for box in boxes:\n",
    "        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "    \n",
    "    # Add to frame list\n",
    "    frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))\n",
    "    '''\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x34504d46/'FMP4' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    }
   ],
   "source": [
    "dim = frames_tracked[0].size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
    "video_tracked = cv2.VideoWriter('video_tracked_2.mp4', fourcc, 25.0, dim)\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n",
    "video_tracked.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('face_recognition')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fdd09f6a44b62e78a4e7154e2dfc62e27023bfdf643f1270facf69ba937d002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
